{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLUq_GVHeaF7"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open(\"names.txt\", \"r\").read().splitlines()"
      ],
      "metadata": {
        "id": "9lnx6bGGecAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E01.  Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?*"
      ],
      "metadata": {
        "id": "Wkr3BWKjyG5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   1. Using counting"
      ],
      "metadata": {
        "id": "Sm1l3-NSyWTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tensor which will hold the trigram counts\n",
        "N = torch.ones((27, 27, 27), dtype=torch.int32) # use torch.ones instead of zeros for \"smoothing\" to avoid inf"
      ],
      "metadata": {
        "id": "qkX117XVfVBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(\"\".join(words)))) # create a sorted list of all characters\n",
        "str_to_int = {s:i + 1 for i, s in enumerate(chars)} # create a mapping for characters to integers\n",
        "str_to_int[\".\"] = 0 # add a special start and end character\n",
        "int_to_str = {i:s for s, i in str_to_int.items()} # create a reverse mapping for ints to characters\n"
      ],
      "metadata": {
        "id": "aZBc7ItbgGql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since we have a dictionary\n",
        "# now we can create trigrams from the dataset (names.txt)\n",
        "\n",
        "for w in words:\n",
        "  chs = [\".\"] + list(w) + [\".\"]\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = str_to_int[ch1]\n",
        "    ix2 = str_to_int[ch2]\n",
        "    ix3 = str_to_int[ch3]\n",
        "    N[ix1, ix2, ix3] += 1 # we add a count for each trigram in each word\n",
        "\n",
        "P = (N).float() # transform the N tensor dtype from int to float\n",
        "P /= P.sum(dim=2, keepdim=True) # normalize the P tensor to have a probability distribution that will equal to one for each slice of the tensor (P[0, 0].sum() == 1)\n",
        "# ^ Reason why we use dim=2 is because we want to normalize the values for each \"3rd\" (index 2) dimension of the tensor:\n",
        "# 0th dimension => rows\n",
        "# 1st dimension => columns\n",
        "# 2nd dimension => \"slice\" of the array (imagine a rubiks cube, the cube is like 3 2 dimensional arrays glued together)\n",
        "# so we normalize each \"slice\" so that each row in each \"slice\" sums to 1\n",
        "\n",
        "# In this step we:\n",
        "# went through all the words,\n",
        "# iterated through trigrams made from those words,\n",
        "# and for every trigram we added a +1 to the N tensor which stores all of the counts for every possible trigram combination"
      ],
      "metadata": {
        "id": "G7KCSNXhgPup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code generates 5 names using our probability distribution tensor P, starting at indexes 0\n",
        "for i in range(5):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    jx = 0\n",
        "    while True:\n",
        "      p1 = P[ix][jx]\n",
        "      # 0 0 5 P[0][0] -> n1\n",
        "      # 0 5 x P[0][n1] -> n2\n",
        "      # 5 x y P[n1][n2] -> n3\n",
        "\n",
        "      next_letter = torch.multinomial(p1, 1, True).item()\n",
        "      ix = jx # 0 -> 0\n",
        "      jx = next_letter # 0 -> n1\n",
        "      out.append(int_to_str[next_letter])\n",
        "      if jx == 0:\n",
        "        break\n",
        "\n",
        "    print(\"\".join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftV2yieWm7t4",
        "outputId": "61cf120e-fc09-4a45-8ec7-bd0206d5d697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kleel.\n",
            "viceuximnie.\n",
            "canahna.\n",
            "ocorgitgoleilebettimaveazwggen.\n",
            "luiree.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate loss (negative log likelihood)\n",
        "\n",
        "log_likelihood = 0\n",
        "n = 0\n",
        "for w in words:\n",
        "  chs = [\".\"] + list(w) + [\".\"]\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    # get the indexes for each trigram\n",
        "    ix1, ix2, ix3 = str_to_int[ch1], str_to_int[ch2], str_to_int[ch3]\n",
        "    # probability for current trigram\n",
        "    prob = P[ix1, ix2, ix3]\n",
        "    # log probability\n",
        "    logprob = torch.log(prob)\n",
        "    # log(a*b*c) = log(a) + log(b) + log(c)\n",
        "    # add to log_likelihood (our loss)\n",
        "    log_likelihood += logprob\n",
        "    # add count for normalization\n",
        "    n+= 1\n",
        "\n",
        "print(f\"{log_likelihood=}\")\n",
        "nll = -log_likelihood\n",
        "print(f\"{nll/n}\") # <- average negative log likelihood (our loss function)"
      ],
      "metadata": {
        "id": "dJAd4Lu3qXVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ddee497-9f2e-49f7-e6f3-af558661481c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_likelihood=tensor(-410414.9688)\n",
            "2.092747449874878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Using NN"
      ],
      "metadata": {
        "id": "xWG5HCYm0irJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KMAR_5vidBTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Summarized ----\n",
        "# create the dataset\n",
        "# initialize the weights\n",
        "# do forward pass\n",
        "# calculate loss\n",
        "# backward propagation using gradient descent\n",
        "\n",
        "\n",
        "def generate_dataset(words):\n",
        "  xs1, ys = [], []\n",
        "  for w in words:\n",
        "    chs = [\".\"] + list(w) + [\".\"]\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "      ix1 = str_to_int[ch1]\n",
        "      ix2 = str_to_int[ch2]\n",
        "      ix3 = str_to_int[ch3]\n",
        "      xs1.append([ix1, ix2])\n",
        "      ys.append(ix3)\n",
        "\n",
        "  xs1 = torch.tensor(xs1, dtype=torch.int64)\n",
        "  ys = torch.tensor(ys, dtype=torch.int64)\n",
        "  return xs1, ys\n",
        "\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtrain, Ytrain = generate_dataset(words[:n1])\n",
        "Xdev, Ydev = generate_dataset(words[n1:n2])\n",
        "Xtest, Ytest = generate_dataset(words[n2:])\n",
        "\n",
        "num = Xtrain.nelement()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FjkvfbKy2DkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create one-hot encodings for first two characters and concatenate them to allow us to pass them in as input to the net\n",
        "Xenc = F.one_hot(Xtrain, num_classes=27).float()\n",
        "Xenc = Xenc.view(-1, (27 * 2))\n",
        "# xenc2 = F.one_hot(Xtrain2, num_classes=27).float()\n",
        "\n",
        "# concat by the 1 dimension to have a tensor in which each row has 27*2 elements\n",
        "# because we want to pass in 2 characters as input and get the prediction for the 3rd character\n",
        "# xenc = torch.cat((xenc1, xenc2), 1)\n",
        "Xenc.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQigqf_zLzWQ",
        "outputId": "f6f3c57e-fee4-4294-a9e0-47a9902cb92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([157152, 54])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of examples: \", num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfkSyPeWOVjH",
        "outputId": "a2bef027-9051-48b6-e283-aa6fe458b48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples:  314304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize weights\n",
        "W = torch.randn((27*2, 27), requires_grad=True) # 27*2 matches the 2nd dimension of our input tensor (196113, 54) @ (54, 27) => (196113, 27)"
      ],
      "metadata": {
        "id": "RRV_gyxsMrtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for k in range(100):\n",
        "  # this is the forward pass\n",
        "  # logits = Xenc @ W # this is passing the input encodings into the neural net and multiplying by the weights\n",
        "  logits = W[Xtrain[:, 0]] + W[Xtrain[:, 1] + 27] # instead of one hot encodings, we just index into the W matrix\n",
        "  # (the reason we add 27 is because we have 54 rows in W, which correspond to 2 input characters so we add 27 to index into the second character embedding)\n",
        "  loss = F.cross_entropy(logits, Ytrain) # we can use cross_entropy instead of calculating the loss manually, more efficient\n",
        "  # counts = logits.exp() # this is getting the counts (equivalent to N tensor which had all the counts of character combinations)\n",
        "  # probs = counts / counts.sum(1, keepdim=True) # normalizing the counts to get probabilities\n",
        "  # loss = -probs[torch.arange(len(Xtrain)), Ytrain].log().mean() # calculating the nll loss\n",
        "  loss += 0.0001*(W**2).mean()  # adding regularization\n",
        "  # print(loss.item())\n",
        "\n",
        "\n",
        "  # now we do the backward pass\n",
        "  W.grad = None # zero out the gradient\n",
        "  loss.backward() # this calculates the gradients for each parameter\n",
        "  W.data += -1 * W.grad # update the parameters in the opposite direction of the gradients\n",
        "\n",
        "  # the \"-\" is because if grad is positive that means that increasing the data will make the loss grow\n",
        "  # so if grad is 0.123 and we update data by 0.01 * 0.123 the loss will grow\n",
        "  # if we do -0.01 * 0.123 we will make data lower which will make the loss lower as well\n",
        "  # if grad is -0.123 that means if data goes up the loss goes down\n",
        "  # so we do -0.01 * -0.123 which makes data go up and the loss goes down\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD9MqrmIPcKC",
        "outputId": "ee59217d-add6-4982-ff69-dccb76012575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.243472099304199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on dev set\n",
        "Xdev_enc = F.one_hot(Xdev, num_classes=27).float()\n",
        "Xdev_enc = Xdev_enc.view(-1, (27*2))\n",
        "for k in range(100):\n",
        "  logits = Xdev_enc @ W\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdim=True)\n",
        "  dev_loss = -probs[torch.arange(len(Xdev)), Ydev].log().mean()\n",
        "\n",
        "  W.grad = None\n",
        "  dev_loss.backward()\n",
        "  W.data += -1 * W.grad\n",
        "print(dev_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1K4HlKGBcrH",
        "outputId": "6f47eb88-968b-4a44-e8f7-90b040fe4fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3925, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on test set\n",
        "Xtest_enc = F.one_hot(Xtest, num_classes=27).float()\n",
        "Xtest_enc = Xtest_enc.view(-1, (27*2))\n",
        "for k in range(1):\n",
        "  logits = Xtest_enc @ W\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdim=True)\n",
        "  test_loss = -probs[torch.arange(len(Xtest)), Ytest].log().mean()\n",
        "\n",
        "print(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDXHL04KDGX6",
        "outputId": "329cc7b0-6614-482a-de1b-0cccb9a1102c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4004, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's test the model by generating some names\n",
        "# This will work similiarly to counting but with some changes\n",
        "for i in range(5):\n",
        "  out = []\n",
        "  ix = 0\n",
        "  jx = 0\n",
        "  while True:\n",
        "    # prepare inputs\n",
        "    xenc1 = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
        "    xenc2 = F.one_hot(torch.tensor([jx]), num_classes=27).float()\n",
        "    xenc = torch.cat((xenc1, xenc2), 1)\n",
        "    # forward pass\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdim=True)\n",
        "    # get the next letter\n",
        "    next_letter = torch.multinomial(probs, 1, True).item()\n",
        "    ix = jx\n",
        "    jx = next_letter\n",
        "    out.append(int_to_str[next_letter])\n",
        "    if jx == 0:\n",
        "      break\n",
        "  print(\"\".join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWsGAk4_ZYcw",
        "outputId": "0bb877fe-fc21-449d-e323-1ae3973ff242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "urimle.\n",
            "van.\n",
            "caiyathyzerhses.\n",
            "urnirn.\n",
            "rle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5nRC5RPjc55k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}